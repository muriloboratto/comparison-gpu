{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fRdbEJCUyJLj",
   "metadata": {
    "id": "fRdbEJCUyJLj"
   },
   "source": [
    "# Comparison Fortran Codes APIs on GPU Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89507cf3-343c-496e-b0ce-1bdfe6ce0435",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Environment Modules on AIRIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae011059-a2b1-48be-b313-4d4b3cf43769",
   "metadata": {},
   "source": [
    "These modules must be initialized before running the jupyter-notebook:\n",
    "```cpp\n",
    "Currently Loaded Modulefiles:\n",
    "    1) anaconda3/2023.07     \n",
    "    2) openmpi/4.1.5  \n",
    "    3) nvhpc/23.11\n",
    "    4) llvm/12.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94595812-2147-4f30-b757-fc41842699a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!module load anaconda3/2023.07 openmpi/4.1.5 nvhpc/23.11 llvm/12.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b9298-1068-4dbf-8744-4a9212723edc",
   "metadata": {},
   "source": [
    "## Module List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48e09af-246f-4e78-9025-f582aee5143a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pgfortran (aka nvfortran) 23.11-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
      "PGI Compilers and Tools\n",
      "Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "!pgfortran --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59781887-2f2b-444a-8a4d-5a8067ebe651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nvfortran 23.11-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
      "NVIDIA Compilers and Tools\n",
      "Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "!nvfortran --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e182c5d-87f8-4d81-8ea2-63c4d263f236",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nvfortran 23.11-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
      "NVIDIA Compilers and Tools\n",
      "Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "!mpif90 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f94abb3-ec09-4d6e-8f1e-51691f01bce1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nvfortran 23.11-0 64-bit target on x86-64 Linux -tp skylake-avx512 \n",
      "NVIDIA Compilers and Tools\n",
      "Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "!mpifort --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a8092-1cda-4fa4-989a-3aaaef3e7ebb",
   "metadata": {},
   "source": [
    "## `Benchmarks MPI + OpenACC + OpenMP `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207f87af-596c-44d5-8b87-1480953881ea",
   "metadata": {},
   "source": [
    "### ⊗ MPI + OpenMP -> mpif90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10e569c-cda0-4238-bfdf-b908825e7835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mpi_openmp.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi_openmp.f90\n",
    "program hello_mpi_openmp\n",
    "  use mpi\n",
    "  implicit none\n",
    "  include 'omp_lib.h'\n",
    "\n",
    "  integer :: ierr, rank, size, thread_id, num_threads, omp_status, omp_provided\n",
    "\n",
    "  ! Initialize MPI\n",
    "  call MPI_Init_thread(MPI_THREAD_FUNNELED, omp_provided, ierr)\n",
    "  call MPI_Comm_rank(MPI_COMM_WORLD, rank, ierr)\n",
    "  call MPI_Comm_size(MPI_COMM_WORLD, size, ierr)\n",
    "\n",
    "  ! Print MPI information\n",
    "  print *, 'Hello from MPI process ', rank, ' of ', size\n",
    "\n",
    "  ! Parallel region with OpenMP\n",
    "  !$omp parallel private(thread_id, num_threads)\n",
    "  num_threads = omp_get_num_threads()\n",
    "  thread_id = omp_get_thread_num()\n",
    "  print *, '  Hello from thread ', thread_id, ' of ', num_threads, ' in MPI process ', rank\n",
    "  !$omp end parallel\n",
    "\n",
    "  ! Finalize MPI\n",
    "  call MPI_Finalize(ierr)\n",
    "\n",
    "end program hello_mpi_openmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5039a748-8dd9-459a-9c72-c6fcd5b6a883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mpif90 mpi_openmp.f90 -o mpi_openmp -fopenmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e94ad9f6-0dc9-4500-8cdb-42932cb34777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello from MPI process             0  of             1\n",
      "   Hello from thread             2  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             9  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             8  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread            11  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             4  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             1  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             7  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread            10  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             3  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             5  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             6  of            12  in MPI process  \n",
      "            0\n",
      "   Hello from thread             0  of            12  in MPI process  \n",
      "            0\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=12 ./mpi_openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916cd60-9ec9-4efa-a70d-d78132efe377",
   "metadata": {},
   "source": [
    "### ⊗ OpenACC + OpenMP -> mpif90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d62aba5-86d3-43fb-8a7b-380eb71e4b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting openmp_openacc.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile openmp_openacc.f90\n",
    "program main\n",
    "    implicit none\n",
    "    integer i, j, n, m\n",
    "    integer, allocatable, dimension(:) :: task\n",
    "    integer, allocatable, dimension(:) :: data\n",
    "\n",
    "    n = 10\n",
    "    allocate(task(n))\n",
    "    do i=1, n\n",
    "        task(i) = i * 1000\n",
    "    end do\n",
    "    print*, 'parallel 10 tasks, each task has 1 GPU kernel'\n",
    "    \n",
    "    !$omp parallel do private(i, m, data) schedule(dynamic)\n",
    "    do j=1, n\n",
    "        m = task(j)\n",
    "        allocate(data(m))\n",
    "\n",
    "        !$acc parallel loop copy(data) async(j)\n",
    "        do i=1, m\n",
    "            data(i) = j\n",
    "        end do\n",
    "        !$acc wait(j)\n",
    "\n",
    "        deallocate(data)\n",
    "    end do\n",
    "    !$omp end parallel do\n",
    "\n",
    "    deallocate(task)\n",
    "    \n",
    "end program main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de8fcabd-432f-4d57-8e1e-2b76e834623c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mpif90 openmp_openacc.f90 -o object -fopenmp -acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0396bcc-f440-4a67-b565-732ee2c0d760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " parallel 10 tasks, each task has 1 GPU kernel\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=8 ./object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b9035-a1c0-4ad2-8008-b7909d58de4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ⊗ MPI + OpenACC -> mpif90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e21105c-5074-4043-813b-2b32449a221f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mpi_openacc.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi_openacc.f90\n",
    "program saxpy_acc_mpi\n",
    "  use openacc\n",
    "  use mpi\n",
    "  implicit none\n",
    "\n",
    "  integer :: len_global, len_local, i\n",
    "  integer :: irank, nranks, igpu, ngpus, ierr, istat(MPI_STATUS_SIZE)\n",
    "  real(4), allocatable, dimension(:) :: X_local, X_global, Y_global, Y_local, Y_ref\n",
    "  real(4) :: a\n",
    "  character(len=128) :: argv\n",
    "\n",
    "  a = 2.0\n",
    "  len_global = 1024\n",
    "\n",
    "  ! Initialize MPI\n",
    "  call MPI_Init(ierr)\n",
    "  call MPI_Comm_rank(MPI_COMM_WORLD, irank, ierr)\n",
    "  call MPI_Comm_size(MPI_COMM_WORLD, nranks, ierr)\n",
    "\n",
    "  ! Check to see that the global array length is evenly divisible by the number of MPI ranks\n",
    "  if (mod(len_global, nranks) .ne. 0) then\n",
    "    if (irank .eq. 0) then\n",
    "      write(*,'(a,5i,a,5i)'), 'The global array length, ', len_global, &\n",
    "          ', is not divisible by the number of ranks, ', nranks\n",
    "      call MPI_Abort(MPI_COMM_WORLD, 1, ierr)\n",
    "    endif\n",
    "  else\n",
    "    len_local = len_global / nranks\n",
    "  endif\n",
    "\n",
    "  ! Find GPU devices and set the device number\n",
    "  ngpus = acc_get_num_devices(acc_device_nvidia)\n",
    "  if (ngpus .le. 0) then\n",
    "    if (irank .eq. 0) then\n",
    "      write(*,'(a)'), 'No NVIDIA GPUs available'\n",
    "      call MPI_Abort(MPI_COMM_WORLD, 1, ierr)\n",
    "    endif\n",
    "  else\n",
    "    igpu = mod(irank, ngpus)\n",
    "    call acc_set_device_num(igpu, acc_device_nvidia)\n",
    "  endif\n",
    "\n",
    "  ! Allocate local and global arrays\n",
    "  allocate(X_local(len_local))\n",
    "  allocate(Y_local(len_local))\n",
    "  if (irank .eq. 0) then\n",
    "    allocate(X_global(len_global))\n",
    "    allocate(Y_global(len_global))\n",
    "    allocate(Y_ref(len_global))\n",
    "  endif\n",
    "\n",
    "  ! If root, set global and reference arrays\n",
    "  if (irank .eq. 0) then\n",
    "    do i = 1, len_global\n",
    "      X_global(i) = i\n",
    "      Y_global(i) = i + len_global\n",
    "      Y_ref(i) = a * i + (i + len_global)\n",
    "    enddo\n",
    "  endif\n",
    "\n",
    "  ! Scatter operands\n",
    "  call MPI_Scatter( &\n",
    "      X_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      X_local, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "  call MPI_Scatter( &\n",
    "      Y_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      Y_local, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "\n",
    "  ! Do local calculation\n",
    "!$ACC KERNELS\n",
    "  do i = 1, len_local\n",
    "    Y_local(i) = a*X_local(i) + Y_local(i)\n",
    "  enddo\n",
    "!$ACC END KERNELS\n",
    "\n",
    "  ! Gather result\n",
    "  call MPI_Gather( &\n",
    "      Y_local, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      Y_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "\n",
    "  ! Root checks result\n",
    "  if (irank .eq. 0) then\n",
    "    print *, 'Ran SAXPY for n = ', len_global\n",
    "    if (all(Y_ref .eq. Y_global)) then\n",
    "      print *, 'SUCCESS: Y_global matches Y_ref'\n",
    "    else\n",
    "      print *, 'FAILURE: Y_global does not match Y_ref'\n",
    "      print *, 'Y_global = ', Y_global\n",
    "      print *, 'Y_ref = ', Y_ref\n",
    "    endif\n",
    "  endif\n",
    "\n",
    "  ! Cleanup\n",
    "  deallocate(X_local)\n",
    "  deallocate(Y_local)\n",
    "  if (irank .eq. 0) then\n",
    "    deallocate(X_global)\n",
    "    deallocate(Y_global)\n",
    "    deallocate(Y_ref)\n",
    "  endif\n",
    "\n",
    "  call MPI_Finalize(ierr)\n",
    "\n",
    "end program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8392bb43-e02b-4f11-acc3-d591e405424a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mpif90 mpi_openacc.f90 -o mpi_openacc -acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0e97d6-1e18-454d-b3f2-75641712e84a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ran SAXPY for n =          1024\n",
      " SUCCESS: Y_global matches Y_ref\n"
     ]
    }
   ],
   "source": [
    "!mpirun -np 2 ./mpi_openacc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df5430-5d0a-46af-a967-a4fa66664b92",
   "metadata": {},
   "source": [
    "### ⊗ MPI + OpenACC + OpenMP -> mpif90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e5cf165-9fb4-4d9a-a224-7ce16bb4f54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpi_openacc_openmp.f90\n"
     ]
    }
   ],
   "source": [
    "%%writefile mpi_openacc_openmp.f90\n",
    "program saxpy_acc_mpi_omp\n",
    "  use openacc\n",
    "  use mpi\n",
    "  implicit none\n",
    "\n",
    "  integer :: len_global, len_local, i\n",
    "  integer :: irank, nranks, igpu, ngpus, ierr, istat(MPI_STATUS_SIZE)\n",
    "  real(4), allocatable, dimension(:) :: X_local, X_global, Y_global, Y_local, Y_ref\n",
    "  real(4), allocatable, dimension(:) :: X_local2, X_global2, Y_global2, Y_local2, Y_ref2\n",
    "  real(4) :: a\n",
    "  character(len=128) :: argv\n",
    "\n",
    "  a = 2.0\n",
    "  len_global = 1024\n",
    "\n",
    "  ! Initialize MPI\n",
    "  call MPI_Init(ierr)\n",
    "  call MPI_Comm_rank(MPI_COMM_WORLD, irank, ierr)\n",
    "  call MPI_Comm_size(MPI_COMM_WORLD, nranks, ierr)\n",
    "\n",
    "  ! Check to see that the global array length is evenly divisible by the number of MPI ranks\n",
    "  if (mod(len_global, nranks) .ne. 0) then\n",
    "    if (irank .eq. 0) then\n",
    "      write(*,'(a,5i,a,5i)'), 'The global array length, ', len_global, &\n",
    "          ', is not divisible by the number of ranks, ', nranks\n",
    "      call MPI_Abort(MPI_COMM_WORLD, 1, ierr)\n",
    "    endif\n",
    "  else\n",
    "    len_local = len_global / nranks\n",
    "  endif\n",
    "\n",
    "  ! Find GPU devices and set the device number\n",
    "  ngpus = acc_get_num_devices(acc_device_nvidia)\n",
    "  if (ngpus .le. 0) then\n",
    "    if (irank .eq. 0) then\n",
    "      write(*,'(a)'), 'No NVIDIA GPUs available'\n",
    "      call MPI_Abort(MPI_COMM_WORLD, 1, ierr)\n",
    "    endif\n",
    "  else\n",
    "    igpu = mod(irank, ngpus)\n",
    "    call acc_set_device_num(igpu, acc_device_nvidia)\n",
    "  endif\n",
    "\n",
    "  ! Allocate local and global arrays\n",
    "  allocate(X_local(len_local))\n",
    "  allocate(X_local2(len_local))\n",
    "  allocate(Y_local(len_local))\n",
    "  allocate(Y_local2(len_local))  \n",
    "  if (irank .eq. 0) then\n",
    "    allocate(X_global(len_global))\n",
    "    allocate(Y_global(len_global))\n",
    "    allocate(Y_ref(len_global))\n",
    "  endif\n",
    "\n",
    "  ! If root, set global and reference arrays\n",
    "  if (irank .eq. 0) then\n",
    "    do i = 1, len_global\n",
    "      X_global(i) = i\n",
    "      Y_global(i) = i + len_global\n",
    "      Y_ref(i) = a * i + (i + len_global)\n",
    "    enddo\n",
    "  endif\n",
    "\n",
    "  ! Scatter operands\n",
    "  call MPI_Scatter( &\n",
    "      X_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      X_local, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "\n",
    "  call MPI_Scatter( &\n",
    "      X_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      X_local2, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "\n",
    "  call MPI_Scatter( &\n",
    "      Y_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      Y_local, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "  \n",
    "  call MPI_Scatter( &\n",
    "      Y_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      Y_local2, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "\n",
    "  ! Do local calculation\n",
    "  !$ACC KERNELS\n",
    "  do i = 1, len_local\n",
    "    Y_local(i) = a*X_local(i) + Y_local(i)\n",
    "  enddo\n",
    "  !$ACC END KERNELS\n",
    "\n",
    "  !$omp parallel do private(i) schedule(dynamic)\n",
    "  do i = 1, len_local\n",
    "    Y_local2(i) = a*X_local2(i) + Y_local2(i)\n",
    "  enddo\n",
    "  !$omp end parallel do  \n",
    "    \n",
    "    \n",
    "  ! Gather result\n",
    "  call MPI_Gather( &\n",
    "      Y_local, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      Y_global, &\n",
    "      len_local, &\n",
    "      MPI_REAL4, &\n",
    "      0, &\n",
    "      MPI_COMM_WORLD, &\n",
    "      ierr &\n",
    "  )\n",
    "\n",
    "  ! Root checks result\n",
    "  if (irank .eq. 0) then\n",
    "    print *, 'Ran SAXPY for n = ', len_global\n",
    "    if (all(Y_ref .eq. Y_global)) then\n",
    "      print *, 'SUCCESS: Y_global matches Y_ref'\n",
    "    else\n",
    "      print *, 'FAILURE: Y_global does not match Y_ref'\n",
    "      print *, 'Y_global = ', Y_global\n",
    "      print *, 'Y_ref = ', Y_ref\n",
    "    endif\n",
    "  endif\n",
    "\n",
    "  ! Cleanup\n",
    "  deallocate(X_local)\n",
    "  deallocate(X_local2)\n",
    "  deallocate(Y_local)\n",
    "  deallocate(Y_local2)  \n",
    "  if (irank .eq. 0) then\n",
    "    deallocate(X_global)\n",
    "    deallocate(Y_global)\n",
    "    deallocate(Y_ref)\n",
    "  endif\n",
    "\n",
    "  call MPI_Finalize(ierr)\n",
    "\n",
    "end program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a93914-f0ca-4bda-99ae-220c36f13b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpif90 mpi_openacc_openmp.f90 -o mpi_openacc_openmp -fopenmp -acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce28203b-35cb-4a4d-9850-81d4dcbf9845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ran SAXPY for n =          1024\n",
      " SUCCESS: Y_global matches Y_ref\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=4 mpirun -np 2 ./mpi_openacc_openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b2f95-d55f-4dd6-ad6c-a34d3e75078d",
   "metadata": {},
   "source": [
    "## Limpando os arquivos remanescentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a478244d-06c8-4588-bae9-01555ccd1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mm* file* fspeed* mpi_openmp* object* hello_* mpi_openacc* openmp_*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FUm5HQBwyJLm",
    "Jq8Zc_8byJLp",
    "bzhJyfWIyJLr",
    "bPQOHun4yJL6"
   ],
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "https://github.com/muriloboratto/SIINTEC2022/blob/master/1-SIINTEC2022-Introduction.ipynb",
     "timestamp": 1666528573400
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
